{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "051d70d956493feee0c6d64651c6a088724dca2a",
    "papermill": {
     "duration": 0.036479,
     "end_time": "2021-02-13T18:19:43.396666",
     "exception": false,
     "start_time": "2021-02-13T18:19:43.360187",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ML for wage prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036639,
     "end_time": "2021-02-13T18:19:43.468425",
     "exception": false,
     "start_time": "2021-02-13T18:19:43.431786",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We illustrate how to predict an outcome variable Y in a high-dimensional setting, where the number of covariates $p$ is large in relation to the sample size $n$. So far we have used linear prediction rules, e.g. Lasso regression, for estimation.\n",
    "Now, we also consider nonlinear prediction rules including tree-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034705,
     "end_time": "2021-02-13T18:19:43.537814",
     "exception": false,
     "start_time": "2021-02-13T18:19:43.503109",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036082,
     "end_time": "2021-02-13T18:19:43.609347",
     "exception": false,
     "start_time": "2021-02-13T18:19:43.573265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Again, we consider data from the U.S. March Supplement of the Current Population Survey (CPS) in 2015.\n",
    "The preproccessed sample consists of $5150$ never-married individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadr\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.279387,
     "end_time": "2021-02-13T18:19:43.923823",
     "exception": false,
     "start_time": "2021-02-13T18:19:43.644436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5150, 20)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdata_read = pyreadr.read_r(\"../data/wage2015_subsample_inference.Rdata\")\n",
    "\n",
    "# Extracting the data frame from rdata_read\n",
    "data = rdata_read[ 'data' ]\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034902,
     "end_time": "2021-02-13T18:19:43.994834",
     "exception": false,
     "start_time": "2021-02-13T18:19:43.959932",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The outcomes $Y_i$'s are hourly (log) wages of never-married workers living in the U.S. The raw regressors $Z_i$'s consist of a variety of characteristics, including experience, education and industry and occupation indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.091723,
     "end_time": "2021-02-13T18:19:44.123394",
     "exception": false,
     "start_time": "2021-02-13T18:19:44.031671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sex', 'shs', 'hsg', 'scl', 'clg', 'ad', 'mw', 'so', 'we', 'ne', 'exp1',\n",
       "       'exp2', 'exp3', 'exp4', 'occ', 'occ2', 'ind', 'ind2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = data.loc[:, 'sex':'ind2']\n",
    "Z.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037074,
     "end_time": "2021-02-13T18:19:44.196749",
     "exception": false,
     "start_time": "2021-02-13T18:19:44.159675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The following figure shows the weekly wage distribution from the US survey data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_hist = [0, 20, 40, 60, 80, 100, 120, 140, 160, 180, 200, 220, 240, 260, 280, 300 , 320, 340, 360, 380, 400, 420, 440, 460, 480, 500, 520, 540]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.443391,
     "end_time": "2021-02-13T18:19:44.677379",
     "exception": false,
     "start_time": "2021-02-13T18:19:44.233988",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 3000.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "C:\\Users\\User\\Documents\\GitHub\\14.388_py\\_build\\jupyter_execute\\Python_notebooks\\08_ML_for_wage_prediction_10_1.png"
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data.wage , bins = np.arange(0, 550, 20) )\n",
    "plt.xlabel('hourly wage')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title( 'Empirical wage distribution from the US survey data' )\n",
    "plt.ylim((0, 3000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036602,
     "end_time": "2021-02-13T18:19:44.752465",
     "exception": false,
     "start_time": "2021-02-13T18:19:44.715863",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Wages show a high degree of skewness. Hence, wages are transformed in almost all studies by\n",
    "the logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036009,
     "end_time": "2021-02-13T18:19:44.826260",
     "exception": false,
     "start_time": "2021-02-13T18:19:44.790251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036925,
     "end_time": "2021-02-13T18:19:44.899159",
     "exception": false,
     "start_time": "2021-02-13T18:19:44.862234",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Due to the skewness of the data, we are considering log wages which leads to the following regression model\n",
    "\n",
    "$$log(wage) = g(Z) + \\epsilon.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036183,
     "end_time": "2021-02-13T18:19:44.971528",
     "exception": false,
     "start_time": "2021-02-13T18:19:44.935345",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will estimate the two sets of prediction rules: Linear and Nonlinear Models.\n",
    "In linear models, we estimate the prediction rule of the form\n",
    "\n",
    "$$\\hat g(Z) = \\hat \\beta'X.$$\n",
    "Again, we generate $X$ in two ways:\n",
    " \n",
    "1. Basic Model:   $X$ consists of a set of raw regressors (e.g. gender, experience, education indicators, regional indicators).\n",
    "\n",
    "\n",
    "2. Flexible Model:  $X$ consists of all raw regressors from the basic model plus occupation and industry indicators, transformations (e.g., ${exp}^2$ and ${exp}^3$) and additional two-way interactions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037318,
     "end_time": "2021-02-13T18:19:45.044959",
     "exception": false,
     "start_time": "2021-02-13T18:19:45.007641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To evaluate the out-of-sample performance, we split the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow = data.shape[0]\n",
    "length = int(nrow*(3/4))\n",
    "data = data.reset_index().drop( 'rownames', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 0.062188,
     "end_time": "2021-02-13T18:19:45.143118",
     "exception": false,
     "start_time": "2021-02-13T18:19:45.080930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "\n",
    "\n",
    "random.seed(30)\n",
    "rng = default_rng()\n",
    "training = rng.choice(nrow, size = length, replace=False)\n",
    "training_bool = data.index.isin( training )\n",
    "\n",
    "data_train = data.iloc[training,:]\n",
    "data_test = data[~training_bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wage</th>\n",
       "      <th>lwage</th>\n",
       "      <th>sex</th>\n",
       "      <th>shs</th>\n",
       "      <th>hsg</th>\n",
       "      <th>scl</th>\n",
       "      <th>clg</th>\n",
       "      <th>ad</th>\n",
       "      <th>mw</th>\n",
       "      <th>so</th>\n",
       "      <th>we</th>\n",
       "      <th>ne</th>\n",
       "      <th>exp1</th>\n",
       "      <th>exp2</th>\n",
       "      <th>exp3</th>\n",
       "      <th>exp4</th>\n",
       "      <th>occ</th>\n",
       "      <th>occ2</th>\n",
       "      <th>ind</th>\n",
       "      <th>ind2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>8.241758</td>\n",
       "      <td>2.109214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.4225</td>\n",
       "      <td>0.274625</td>\n",
       "      <td>0.178506</td>\n",
       "      <td>2300</td>\n",
       "      <td>8</td>\n",
       "      <td>8470</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>9.615385</td>\n",
       "      <td>2.263364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>8140</td>\n",
       "      <td>21</td>\n",
       "      <td>3570</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>16.826923</td>\n",
       "      <td>2.822980</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3160</td>\n",
       "      <td>10</td>\n",
       "      <td>8190</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4123</th>\n",
       "      <td>7.211538</td>\n",
       "      <td>1.975682</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>3535</td>\n",
       "      <td>10</td>\n",
       "      <td>8290</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4118</th>\n",
       "      <td>16.826923</td>\n",
       "      <td>2.822980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>7200</td>\n",
       "      <td>20</td>\n",
       "      <td>7080</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3987</th>\n",
       "      <td>26.923077</td>\n",
       "      <td>3.292984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6.7600</td>\n",
       "      <td>17.576000</td>\n",
       "      <td>45.697600</td>\n",
       "      <td>4840</td>\n",
       "      <td>16</td>\n",
       "      <td>7390</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2581</th>\n",
       "      <td>36.057692</td>\n",
       "      <td>3.585120</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.6100</td>\n",
       "      <td>6.859000</td>\n",
       "      <td>13.032100</td>\n",
       "      <td>5400</td>\n",
       "      <td>17</td>\n",
       "      <td>4670</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>13.461538</td>\n",
       "      <td>2.599837</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.9600</td>\n",
       "      <td>2.744000</td>\n",
       "      <td>3.841600</td>\n",
       "      <td>4200</td>\n",
       "      <td>14</td>\n",
       "      <td>7690</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3645</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>9640</td>\n",
       "      <td>22</td>\n",
       "      <td>4970</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>3.912023</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.9600</td>\n",
       "      <td>2.744000</td>\n",
       "      <td>3.841600</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>7570</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3862 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           wage     lwage  sex  shs  hsg  scl  clg   ad   mw   so   we   ne  \\\n",
       "2294   8.241758  2.109214  1.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "2207   9.615385  2.263364  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "1087  16.826923  2.822980  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0   \n",
       "4123   7.211538  1.975682  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
       "4118  16.826923  2.822980  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
       "...         ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "3987  26.923077  3.292984  1.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "2581  36.057692  3.585120  1.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0   \n",
       "1619  13.461538  2.599837  1.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "3645  12.000000  2.484907  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "1263  50.000000  3.912023  1.0  0.0  0.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0   \n",
       "\n",
       "      exp1     exp2       exp3        exp4   occ occ2   ind ind2  \n",
       "2294   6.5   0.4225   0.274625    0.178506  2300    8  8470   18  \n",
       "2207   5.0   0.2500   0.125000    0.062500  8140   21  3570    6  \n",
       "1087   0.0   0.0000   0.000000    0.000000  3160   10  8190   18  \n",
       "4123  20.0   4.0000   8.000000   16.000000  3535   10  8290   18  \n",
       "4118  40.0  16.0000  64.000000  256.000000  7200   20  7080   13  \n",
       "...    ...      ...        ...         ...   ...  ...   ...  ...  \n",
       "3987  26.0   6.7600  17.576000   45.697600  4840   16  7390   14  \n",
       "2581  19.0   3.6100   6.859000   13.032100  5400   17  4670    9  \n",
       "1619  14.0   1.9600   2.744000    3.841600  4200   14  7690   16  \n",
       "3645   5.0   0.2500   0.125000    0.062500  9640   22  4970    9  \n",
       "1263  14.0   1.9600   2.744000    3.841600   136    1  7570   15  \n",
       "\n",
       "[3862 rows x 20 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3862, 20)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038774,
     "end_time": "2021-02-13T18:19:45.217757",
     "exception": false,
     "start_time": "2021-02-13T18:19:45.178983",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We construct the two different model matrices $X_{basic}$ and $X_{flex}$ for both the training and the test sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import patsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "papermill": {
     "duration": 0.094135,
     "end_time": "2021-02-13T18:19:45.347955",
     "exception": false,
     "start_time": "2021-02-13T18:19:45.253820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "formula_basic =  \"lwage ~ sex + exp1 + exp2+ shs + hsg+ scl + clg + mw + so + we + occ2+ ind2\"\n",
    "formula_flex = \"lwage ~ sex + exp1 + exp2 + shs+hsg+scl+clg+occ2+ind2+mw+so+we + (exp1+exp2+exp3+exp4)*(shs+hsg+scl+clg+occ2+ind2+mw+so+we)\"\n",
    "\n",
    "y_basic_train, model_X_basic_train = patsy.dmatrices(formula_basic, data_train, return_type='dataframe')\n",
    "y_basic_test, model_X_basic_test = patsy.dmatrices(formula_basic, data_test, return_type='dataframe')\n",
    "p_basic = model_X_basic_train.shape[ 1 ]\n",
    "\n",
    "y_flex_train, model_X_flex_train = patsy.dmatrices(formula_flex, data_train, return_type='dataframe')\n",
    "y_flex_test, model_X_flex_test = patsy.dmatrices(formula_flex, data_test, return_type='dataframe')\n",
    "p_flex = model_X_flex_train.shape[ 1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>occ2[T.10]</th>\n",
       "      <th>occ2[T.11]</th>\n",
       "      <th>occ2[T.12]</th>\n",
       "      <th>occ2[T.13]</th>\n",
       "      <th>occ2[T.14]</th>\n",
       "      <th>occ2[T.15]</th>\n",
       "      <th>occ2[T.16]</th>\n",
       "      <th>occ2[T.17]</th>\n",
       "      <th>occ2[T.18]</th>\n",
       "      <th>...</th>\n",
       "      <th>sex</th>\n",
       "      <th>exp1</th>\n",
       "      <th>exp2</th>\n",
       "      <th>shs</th>\n",
       "      <th>hsg</th>\n",
       "      <th>scl</th>\n",
       "      <th>clg</th>\n",
       "      <th>mw</th>\n",
       "      <th>so</th>\n",
       "      <th>we</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.4225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4123</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4118</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3987</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6.7600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2581</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.6100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.9600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3645</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.9600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3862 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Intercept  occ2[T.10]  occ2[T.11]  occ2[T.12]  occ2[T.13]  occ2[T.14]  \\\n",
       "2294        1.0         0.0         0.0         0.0         0.0         0.0   \n",
       "2207        1.0         0.0         0.0         0.0         0.0         0.0   \n",
       "1087        1.0         1.0         0.0         0.0         0.0         0.0   \n",
       "4123        1.0         1.0         0.0         0.0         0.0         0.0   \n",
       "4118        1.0         0.0         0.0         0.0         0.0         0.0   \n",
       "...         ...         ...         ...         ...         ...         ...   \n",
       "3987        1.0         0.0         0.0         0.0         0.0         0.0   \n",
       "2581        1.0         0.0         0.0         0.0         0.0         0.0   \n",
       "1619        1.0         0.0         0.0         0.0         0.0         1.0   \n",
       "3645        1.0         0.0         0.0         0.0         0.0         0.0   \n",
       "1263        1.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "      occ2[T.15]  occ2[T.16]  occ2[T.17]  occ2[T.18]  ...  sex  exp1     exp2  \\\n",
       "2294         0.0         0.0         0.0         0.0  ...  1.0   6.5   0.4225   \n",
       "2207         0.0         0.0         0.0         0.0  ...  0.0   5.0   0.2500   \n",
       "1087         0.0         0.0         0.0         0.0  ...  1.0   0.0   0.0000   \n",
       "4123         0.0         0.0         0.0         0.0  ...  0.0  20.0   4.0000   \n",
       "4118         0.0         0.0         0.0         0.0  ...  0.0  40.0  16.0000   \n",
       "...          ...         ...         ...         ...  ...  ...   ...      ...   \n",
       "3987         0.0         1.0         0.0         0.0  ...  1.0  26.0   6.7600   \n",
       "2581         0.0         0.0         1.0         0.0  ...  1.0  19.0   3.6100   \n",
       "1619         0.0         0.0         0.0         0.0  ...  1.0  14.0   1.9600   \n",
       "3645         0.0         0.0         0.0         0.0  ...  0.0   5.0   0.2500   \n",
       "1263         0.0         0.0         0.0         0.0  ...  1.0  14.0   1.9600   \n",
       "\n",
       "      shs  hsg  scl  clg   mw   so   we  \n",
       "2294  0.0  0.0  1.0  0.0  1.0  0.0  0.0  \n",
       "2207  0.0  1.0  0.0  0.0  1.0  0.0  0.0  \n",
       "1087  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4123  0.0  1.0  0.0  0.0  0.0  0.0  1.0  \n",
       "4118  0.0  0.0  1.0  0.0  0.0  0.0  1.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "3987  0.0  1.0  0.0  0.0  0.0  1.0  0.0  \n",
       "2581  0.0  0.0  0.0  1.0  0.0  1.0  0.0  \n",
       "1619  0.0  1.0  0.0  0.0  1.0  0.0  0.0  \n",
       "3645  0.0  1.0  0.0  0.0  0.0  1.0  0.0  \n",
       "1263  0.0  0.0  0.0  1.0  1.0  0.0  0.0  \n",
       "\n",
       "[3862 rows x 52 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_X_basic_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "papermill": {
     "duration": 0.060969,
     "end_time": "2021-02-13T18:19:45.445389",
     "exception": false,
     "start_time": "2021-02-13T18:19:45.384420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_train = data_train['lwage']\n",
    "Y_test = data_test['lwage']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "papermill": {
     "duration": 0.062723,
     "end_time": "2021-02-13T18:19:45.545189",
     "exception": false,
     "start_time": "2021-02-13T18:19:45.482466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "246\n"
     ]
    }
   ],
   "source": [
    "print(p_basic)\n",
    "print(p_flex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037704,
     "end_time": "2021-02-13T18:19:45.622370",
     "exception": false,
     "start_time": "2021-02-13T18:19:45.584666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As known from our first lab, the basic model consists of $10$ regressors and the flexible model of $246$ regressors. Let us fit our models to the training sample using the two different model specifications. We are starting by running a simple ols regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038763,
     "end_time": "2021-02-13T18:19:45.699126",
     "exception": false,
     "start_time": "2021-02-13T18:19:45.660363",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039458,
     "end_time": "2021-02-13T18:19:45.779460",
     "exception": false,
     "start_time": "2021-02-13T18:19:45.740002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We fit the basic model to our training data by running an ols regression and compute the mean squared error on the test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "papermill": {
     "duration": 0.069537,
     "end_time": "2021-02-13T18:19:45.887169",
     "exception": false,
     "start_time": "2021-02-13T18:19:45.817632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error (MSE) using the basic model is equal to , 0.22150006331940958 \n"
     ]
    }
   ],
   "source": [
    "# ols (basic model)\n",
    "lm_basic = sm.OLS( Y_train, model_X_basic_train )\n",
    "fit_lm_basic = lm_basic.fit()\n",
    "\n",
    "# Compute the Out-Of-Sample Performance\n",
    "yhat_lm_basic = fit_lm_basic.predict( model_X_basic_test )\n",
    "print( f\"The mean squared error (MSE) using the basic model is equal to , {np.mean((Y_test-yhat_lm_basic)**2)} \") # MSE OLS (basic model)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-13T18:19:45.861949Z",
     "iopub.status.busy": "2021-02-13T18:19:45.860219Z",
     "iopub.status.idle": "2021-02-13T18:19:45.887011Z",
     "shell.execute_reply": "2021-02-13T18:19:45.885343Z"
    },
    "papermill": {
     "duration": 0.069537,
     "end_time": "2021-02-13T18:19:45.887169",
     "exception": false,
     "start_time": "2021-02-13T18:19:45.817632",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To determine the out-of-sample $MSE$ and the standard error in one step, we can use the function *lm*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "papermill": {
     "duration": 0.069537,
     "end_time": "2021-02-13T18:19:45.887169",
     "exception": false,
     "start_time": "2021-02-13T18:19:45.817632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Coef.       0.221500\n",
       "Std.Err.    0.011458\n",
       "Name: const, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resid_basic = (Y_test-yhat_lm_basic)**2\n",
    "\n",
    "MSE_lm_basic = sm.OLS( resid_basic , np.ones( resid_basic.shape[0] ) ).fit().summary2().tables[1].iloc[0, 0:2]\n",
    "MSE_lm_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-13T18:19:45.861949Z",
     "iopub.status.busy": "2021-02-13T18:19:45.860219Z",
     "iopub.status.idle": "2021-02-13T18:19:45.887011Z",
     "shell.execute_reply": "2021-02-13T18:19:45.885343Z"
    },
    "papermill": {
     "duration": 0.069537,
     "end_time": "2021-02-13T18:19:45.887169",
     "exception": false,
     "start_time": "2021-02-13T18:19:45.817632",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We also compute the out-of-sample $R^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "papermill": {
     "duration": 0.069537,
     "end_time": "2021-02-13T18:19:45.887169",
     "exception": false,
     "start_time": "2021-02-13T18:19:45.817632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R^2 using the basic model is equal to, 0.30183042074937083\n"
     ]
    }
   ],
   "source": [
    "R2_lm_basic = 1 - ( MSE_lm_basic[0]/Y_test.var() )\n",
    "print( f\"The R^2 using the basic model is equal to, {R2_lm_basic}\" ) # MSE OLS (basic model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039585,
     "end_time": "2021-02-13T18:19:46.492903",
     "exception": false,
     "start_time": "2021-02-13T18:19:46.453318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We repeat the same procedure for the flexible model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R^2 using the flex model is equal to, 0.21847497396574378\n"
     ]
    }
   ],
   "source": [
    "# ols (flex model)\n",
    "lm_flex = sm.OLS( Y_train, model_X_flex_train )\n",
    "fit_lm_flex = lm_flex.fit()\n",
    "\n",
    "yhat_lm_flex = fit_lm_flex.predict( model_X_flex_test )\n",
    "\n",
    "resid_flex = (Y_test-yhat_lm_flex)**2\n",
    "\n",
    "MSE_lm_flex = sm.OLS( resid_flex , np.ones( resid_flex.shape[0] ) ).fit().summary2().tables[1].iloc[0, 0:2]\n",
    "MSE_lm_flex\n",
    "\n",
    "R2_lm_flex = 1 - ( MSE_lm_flex[0]/Y_test.var() )\n",
    "print( f\"The R^2 using the flex model is equal to, {R2_lm_flex}\" ) # MSE OLS (flex model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.051953,
     "end_time": "2021-02-13T18:19:46.853182",
     "exception": false,
     "start_time": "2021-02-13T18:19:46.801229",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We observe that ols regression works better for the basic model with smaller $p/n$ ratio. We are proceeding by running lasso regressions and its versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042521,
     "end_time": "2021-02-13T18:19:46.935859",
     "exception": false,
     "start_time": "2021-02-13T18:19:46.893338",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Lasso, Ridge and Elastic Net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040161,
     "end_time": "2021-02-13T18:19:47.015626",
     "exception": false,
     "start_time": "2021-02-13T18:19:46.975465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Considering the basic model, we run a lasso/post-lasso regression first and then we compute the measures for the out-of-sample performance. Note that applying the package *hdm* and the function *rlasso* we rely on a theoretical based choice of the penalty level $\\lambda$ in the lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdmpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_rlasso = hdmpy.rlasso( model_X_basic_train.to_numpy() , Y_train.to_numpy().reshape( Y_train.size , 1 ) , post = False )\n",
    "fit_rlasso_post = hdmpy.rlasso( model_X_basic_train.to_numpy() , Y_train.to_numpy().reshape( Y_train.size , 1 ) , post = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating the predictions from rlasso models\n",
    "We have to know that the residuals output come from this formula:\n",
    "\n",
    "- x1 = x - np.ones( (x.shape[1] , 1) ) @ x.mean( axis = 0 )\n",
    "- beta = model.est['beta'].loc[ fit_rlasso.est['index'].iloc[:, 0].to_list(), ].to_numpy()\n",
    "- y1 = y - y.mean()\n",
    "- yhat = x1 @ beta + y.mean()\n",
    "\n",
    "So we have to apply those transfomations to original test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting mean of each variable\n",
    "meanx = model_X_basic_test.mean( axis = 0 ).values.\\\n",
    "                        reshape( model_X_basic_test.shape[ 1 ] , 1 )\n",
    "\n",
    "# Reducing the mean\n",
    "new_x1 = model_X_basic_test.to_numpy() - \\\n",
    "                    (np.ones( ( model_X_basic_test.shape[ 0 ] , 1 ) ) @ meanx.T)\n",
    "\n",
    "# Getting the significant variables\n",
    "x1_est_rlasso = new_x1[ :, fit_rlasso.est['index'].iloc[:, 0].to_list()]\n",
    "\n",
    "# Getting the coef. from significant variables\n",
    "beta_rlasso = fit_rlasso.est['beta'].loc[ fit_rlasso.est['index'].\\\n",
    "                                     iloc[:, 0].to_list(), ].to_numpy()\n",
    "\n",
    "# yhat\n",
    "yhat_rlasso = (x1_est_rlasso @ beta_rlasso) + np.mean( Y_test.to_numpy() )\n",
    "residuals_rlasso = Y_test.to_numpy().reshape( Y_test.to_numpy().size, 1)  - yhat_rlasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting mean of each variable\n",
    "meanx = model_X_basic_test.mean( axis = 0 ).values.\\\n",
    "                        reshape( model_X_basic_test.shape[ 1 ] , 1 )\n",
    "\n",
    "# Reducing the mean\n",
    "new_x1 = model_X_basic_test.to_numpy() - \\\n",
    "                    (np.ones( ( model_X_basic_test.shape[ 0 ] , 1 ) ) @ meanx.T)\n",
    "\n",
    "# Getting the significant variables\n",
    "x1_est_rlasso_post = new_x1[ :, fit_rlasso_post.est['index'].iloc[:, 0].to_list()]\n",
    "\n",
    "# Getting the coef. from significant variables\n",
    "beta_rlasso_post = fit_rlasso_post.est['beta'].loc[ fit_rlasso_post.est['index'].\\\n",
    "                                     iloc[:, 0].to_list(), ].to_numpy()\n",
    "\n",
    "# yhat\n",
    "yhat_rlasso_post = (x1_est_rlasso_post @ beta_rlasso_post) + np.mean( Y_test.to_numpy() )\n",
    "residuals_rlasso_post = Y_test.to_numpy().reshape( Y_test.to_numpy().size, 1)  - yhat_rlasso_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R^2 using the basic model is equal to 0.261857450431617,for lasso and 0.2650119057716528 for post-lasso\n"
     ]
    }
   ],
   "source": [
    "MSE_lasso = sm.OLS( ( residuals_rlasso )**2 , np.ones( yhat_rlasso.size )  ).fit().summary2().tables[1].round(3)\n",
    "MSE_lasso_post = sm.OLS( ( residuals_rlasso_post )**2  , np.ones( yhat_rlasso_post.size )  ).fit().summary2().tables[1].round(3)\n",
    "\n",
    "R2_lasso = 1 - MSE_lasso.iloc[0, 0]/ np.var( Y_test )\n",
    "R2_lasso_post = 1 - MSE_lasso_post.iloc[0, 0]/ np.var( Y_test )\n",
    "\n",
    "print( f\"The R^2 using the basic model is equal to {R2_lasso},for lasso and {R2_lasso_post} for post-lasso\") # R^2 lasso/post-lasso (basic model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.049543,
     "end_time": "2021-02-13T18:19:47.757271",
     "exception": false,
     "start_time": "2021-02-13T18:19:47.707728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, we repeat the same procedure for the flexible model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_rlasso_flex = hdmpy.rlasso( model_X_flex_train.to_numpy() , Y_train.to_numpy().reshape( Y_train.size , 1 ) , post = False )\n",
    "fit_rlasso_post_flex = hdmpy.rlasso( model_X_flex_train.to_numpy() , Y_train.to_numpy().reshape( Y_train.size , 1 ) , post = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting mean of each variable\n",
    "meanx = model_X_flex_test.mean( axis = 0 ).values.\\\n",
    "                        reshape( model_X_flex_test.shape[ 1 ] , 1 )\n",
    "\n",
    "# Reducing the mean\n",
    "new_x1 = model_X_flex_test.to_numpy() - \\\n",
    "                    (np.ones( ( model_X_flex_test.shape[ 0 ] , 1 ) ) @ meanx.T)\n",
    "\n",
    "# Getting the significant variables\n",
    "x1_est_rlasso_flex = new_x1[ :, fit_rlasso_flex.est['index'].iloc[:, 0].to_list()]\n",
    "\n",
    "# Getting the coef. from significant variables\n",
    "beta_rlasso_flex = fit_rlasso_flex.est['beta'].loc[ fit_rlasso_flex.est['index'].\\\n",
    "                                     iloc[:, 0].to_list(), ].to_numpy()\n",
    "\n",
    "# yhat\n",
    "yhat_rlasso_flex = (x1_est_rlasso_flex @ beta_rlasso_flex) + np.mean( Y_test.to_numpy() )\n",
    "residuals_rlasso_flex = Y_test.to_numpy().reshape( Y_test.to_numpy().size, 1)  - yhat_rlasso_flex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting mean of each variable\n",
    "meanx = model_X_flex_test.mean( axis = 0 ).values.\\\n",
    "                        reshape( model_X_flex_test.shape[ 1 ] , 1 )\n",
    "\n",
    "# Reducing the mean\n",
    "new_x1 = model_X_flex_test.to_numpy() - \\\n",
    "                    (np.ones( ( model_X_flex_test.shape[ 0 ] , 1 ) ) @ meanx.T)\n",
    "\n",
    "# Getting the significant variables\n",
    "x1_est_rlasso_post_flex = new_x1[ :, fit_rlasso_post_flex.est['index'].iloc[:, 0].to_list()]\n",
    "\n",
    "# Getting the coef. from significant variables\n",
    "beta_rlasso_post_flex = fit_rlasso_post_flex.est['beta'].loc[ fit_rlasso_post_flex.est['index'].\\\n",
    "                                     iloc[:, 0].to_list(), ].to_numpy()\n",
    "\n",
    "# yhat\n",
    "yhat_rlasso_post_flex = (x1_est_rlasso_post_flex @ beta_rlasso_post_flex) + np.mean( Y_test.to_numpy() )\n",
    "residuals_rlasso_post_flex = Y_test.to_numpy().reshape( Y_test.to_numpy().size, 1)  - yhat_rlasso_post_flex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R^2 using the basic model is equal to 0.261857450431617,for lasso and 0.261857450431617 for post-lasso\n"
     ]
    }
   ],
   "source": [
    "MSE_lasso_flex = sm.OLS( ( residuals_rlasso_flex )**2 , np.ones( yhat_rlasso_flex.size )  ).fit().summary2().tables[1].round(3)\n",
    "MSE_lasso_post_flex = sm.OLS( ( residuals_rlasso_post_flex )**2  , np.ones( yhat_rlasso_post_flex.size )  ).fit().summary2().tables[1].round(3)\n",
    "\n",
    "R2_lasso_flex = 1 - MSE_lasso.iloc[0, 0]/ np.var( Y_test )\n",
    "R2_lasso_post_flex = 1 - MSE_lasso_post_flex.iloc[0, 0]/ np.var( Y_test )\n",
    "\n",
    "print( f\"The R^2 using the basic model is equal to {R2_lasso_flex},for lasso and {R2_lasso_post_flex} for post-lasso\") # R^2 lasso/post-lasso (basic model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04206,
     "end_time": "2021-02-13T18:19:51.353816",
     "exception": false,
     "start_time": "2021-02-13T18:19:51.311756",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It is worth to notice that lasso regression works better for the more complex model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041452,
     "end_time": "2021-02-13T18:19:51.436401",
     "exception": false,
     "start_time": "2021-02-13T18:19:51.394949",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In contrast to a theoretical based choice of the tuning parameter $\\lambda$ in the lasso regression, we can also use cross-validation to determine the penalty level by applying the package *glmnet* and the function cv.glmnet. In this context, we also run a ridge and a elastic net regression by adjusting the parameter *alpha*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV, ElasticNetCV\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping Y variable\n",
    "Y_vec = Y_train.to_numpy().reshape( Y_train.to_numpy().size, 1)\n",
    "\n",
    "# Scalar distribution\n",
    "scaler = StandardScaler()\n",
    "scaler.fit( Y_vec )\n",
    "std_Y = scaler.transform( Y_vec )\n",
    "\n",
    "# Regressions\n",
    "fit_lasso_cv_basic = LassoCV(cv = 10 , random_state = 0 , normalize = True ).fit( model_X_basic_train, std_Y )\n",
    "fit_ridge_basic = ElasticNetCV( cv = 10 , normalize = True , random_state = 0 , l1_ratio = 0.0001 ).fit( model_X_basic_train , std_Y )\n",
    "fit_elnet_basic = ElasticNetCV( cv = 10 , normalize = True , random_state = 0 , l1_ratio = 0.5, max_iter = 100000 ).fit( model_X_basic_train , std_Y )\n",
    "\n",
    "# Predictions\n",
    "yhat_lasso_cv_basic = scaler.inverse_transform( fit_lasso_cv_basic.predict( model_X_basic_test ).reshape(-1,1) )\n",
    "yhat_ridge_basic = scaler.inverse_transform( fit_ridge_basic.predict( model_X_basic_test ).reshape(-1,1) )\n",
    "yhat_elnet_basic = scaler.inverse_transform( fit_elnet_basic.predict( model_X_basic_test ).reshape(-1,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_lasso_cv_basic = sm.OLS( ((Y_test.to_numpy().reshape(-1,1) - yhat_lasso_cv_basic)**2 ) , np.ones( yhat_lasso_cv_basic.shape )  ).fit().summary2().tables[1].round(3)\n",
    "MSE_ridge_basic = sm.OLS( ((Y_test.to_numpy().reshape(-1,1) - yhat_ridge_basic)**2 ) , np.ones( yhat_ridge_basic.size )  ).fit().summary2().tables[1].round(3)\n",
    "MSE_elnet_basic = sm.OLS( ((Y_test.to_numpy().reshape(-1,1) - yhat_elnet_basic)**2 ) , np.ones( yhat_elnet_basic.size )  ).fit().summary2().tables[1].round(3)\n",
    "# our coefficient of MSE_elnet are far from r output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_lasso_cv_basic = 1- MSE_ridge_basic.iloc[0,0] / np.var( Y_test )\n",
    "R2_ridge_basic = 1- MSE_lasso_cv_basic.iloc[0,0] / np.var( Y_test )\n",
    "R2_elnet_basic = 1- MSE_elnet_basic.iloc[0,0] / np.var( Y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 using cross-validation for lasso, ridge and elastic net in the basic model: 0.006346567888715238,0.2997109145120469,0.2997109145120469\n"
     ]
    }
   ],
   "source": [
    "print( f\"R^2 using cross-validation for lasso, ridge and elastic net in the basic model: {R2_lasso_cv_basic},{R2_ridge_basic},{R2_elnet_basic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping Y variable\n",
    "Y_vec = Y_train.to_numpy().reshape( Y_train.to_numpy().size, 1)\n",
    "\n",
    "# Scalar distribution\n",
    "scaler = StandardScaler()\n",
    "scaler.fit( Y_vec )\n",
    "std_Y = scaler.transform( Y_vec )\n",
    "\n",
    "# Regressions\n",
    "fit_lasso_cv_flex = LassoCV(cv = 10 , random_state = 0 , normalize = True ).fit( model_X_flex_train, std_Y )\n",
    "fit_ridge_flex = ElasticNetCV( cv = 10 , normalize = True , random_state = 0 , l1_ratio = 0.0001 ).fit( model_X_flex_train , std_Y )\n",
    "fit_elnet_flex = ElasticNetCV( cv = 10 , normalize = True , random_state = 0 , l1_ratio = 0.5, max_iter = 100000 ).fit( model_X_flex_train , std_Y )\n",
    "\n",
    "# Predictions\n",
    "yhat_lasso_cv_flex = scaler.inverse_transform( fit_lasso_cv_flex.predict( model_X_flex_test ).reshape(-1,1) )\n",
    "yhat_ridge_flex = scaler.inverse_transform( fit_ridge_flex.predict( model_X_flex_test ).reshape(-1,1) )\n",
    "yhat_elnet_flex = scaler.inverse_transform( fit_elnet_flex.predict( model_X_flex_test ).reshape(-1,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_lasso_cv_flex = sm.OLS( ((Y_test.to_numpy().reshape(-1,1) - yhat_lasso_cv_flex)**2 ) , np.ones( yhat_lasso_cv_flex.shape )  ).fit().summary2().tables[1].round(3)\n",
    "MSE_ridge_flex = sm.OLS( ((Y_test.to_numpy().reshape(-1,1) - yhat_ridge_flex)**2 ) , np.ones( yhat_ridge_flex.size )  ).fit().summary2().tables[1].round(3)\n",
    "MSE_elnet_flex = sm.OLS( ((Y_test.to_numpy().reshape(-1,1) - yhat_elnet_flex)**2 ) , np.ones( yhat_elnet_flex.size )  ).fit().summary2().tables[1].round(3)\n",
    "# our coefficient of MSE_elnet are far from r output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_lasso_cv_flex = 1- MSE_ridge_flex.iloc[0,0] / np.var( Y_test )\n",
    "R2_ridge_flex = 1- MSE_lasso_cv_flex.iloc[0,0] / np.var( Y_test )\n",
    "R2_elnet_flex = 1- MSE_elnet_flex.iloc[0,0] / np.var( Y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 using cross-validation for lasso, ridge and elastic net in the basic model: 0.012655478568786904,0.29024754849193946,0.2839386378118678\n"
     ]
    }
   ],
   "source": [
    "print( f\"R^2 using cross-validation for lasso, ridge and elastic net in the basic model: {R2_lasso_cv_flex},{R2_ridge_flex},{R2_elnet_flex}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04263,
     "end_time": "2021-02-13T18:20:07.529566",
     "exception": false,
     "start_time": "2021-02-13T18:20:07.486936",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The performance of the lasso regression with cross-validated penalty is quite similar to the performance of lasso using a theoretical based choice of the tuning parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042859,
     "end_time": "2021-02-13T18:20:07.614751",
     "exception": false,
     "start_time": "2021-02-13T18:20:07.571892",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Non-linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042125,
     "end_time": "2021-02-13T18:20:07.699092",
     "exception": false,
     "start_time": "2021-02-13T18:20:07.656967",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Besides linear regression models, we consider nonlinear regression models to build a predictive model. We are applying regression trees, random forests, boosted trees and neural nets to estimate the regression function $g(X)$. First, we load the relevant libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.043418,
     "end_time": "2021-02-13T18:20:08.174622",
     "exception": false,
     "start_time": "2021-02-13T18:20:08.131204",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "and we illustrate the application of regression trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.043267,
     "end_time": "2021-02-13T18:20:08.261600",
     "exception": false,
     "start_time": "2021-02-13T18:20:08.218333",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.043445,
     "end_time": "2021-02-13T18:20:08.348402",
     "exception": false,
     "start_time": "2021-02-13T18:20:08.304957",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We fit a regression tree to the training data using the basic model. The variable *cp* controls the complexity of the regression tree, i.e. how deep we build the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import tree\n",
    "from scipy.sparse import diags\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cp** = It is the amout by which splitting that node would decarease the relative error.\\\n",
    "It has the same meaning as min_impurity_decrease \n",
    "Apparently, Sklearn does not have tree prune function as stated in theis user guide. I take the info from [this link](https://stats.stackexchange.com/questions/152553/what-is-the-equivalent-of-the-complexity-parameter-rpart-in-r-in-python-for)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can Preprune and postprune decission trees.\n",
    "\n",
    "Preprunning is stopping the growth of decision tree on an early stage. we can limit paramters like max_depth, min_samples. We can grid search those parameters and choose the optimum values that gives better performance on test data. \n",
    "\n",
    "Cost complexity pruning\\\n",
    "It is all about finding the right parameter for alpha. We will get the alpha values for this tree\n",
    "\n",
    "we are going to cut some threes in order to not overfitting data. We will calculate the total sum of squared residuals from each leaf of each type of three and store that results.\n",
    "\n",
    "how to compare these threes?\n",
    "\n",
    "Tree Scores = SSR + alpha(Number of leafs) We will penalize for each additional three.\n",
    "We will get the alpha value from cross validation. We can check the code [here](https://www.kaggle.com/arunmohan003/pruning-decision-trees)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cp = It is the minimum value that the R-squared should decrease in order to make the next splitting \\\n",
    "Xerror = Cross-Validated Error Rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = DecisionTreeRegressor( random_state = 0, min_impurity_decrease = 0.001 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ccp_alphas</th>\n",
       "      <th>impurities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.243664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001094</td>\n",
       "      <td>2.244758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001269</td>\n",
       "      <td>2.246027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001350</td>\n",
       "      <td>2.247377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001364</td>\n",
       "      <td>2.248741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001372</td>\n",
       "      <td>2.250113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001386</td>\n",
       "      <td>2.251499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001403</td>\n",
       "      <td>2.252903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001438</td>\n",
       "      <td>2.254340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001444</td>\n",
       "      <td>2.258672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.001586</td>\n",
       "      <td>2.261844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.001636</td>\n",
       "      <td>2.265116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.001676</td>\n",
       "      <td>2.266792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.001693</td>\n",
       "      <td>2.268485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.001702</td>\n",
       "      <td>2.270187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.001727</td>\n",
       "      <td>2.273642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.001732</td>\n",
       "      <td>2.275374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.001767</td>\n",
       "      <td>2.277141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.001790</td>\n",
       "      <td>2.278931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.001838</td>\n",
       "      <td>2.280769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.001909</td>\n",
       "      <td>2.282678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.001912</td>\n",
       "      <td>2.286501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.001947</td>\n",
       "      <td>2.290395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.002016</td>\n",
       "      <td>2.292411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.002025</td>\n",
       "      <td>2.296462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.002028</td>\n",
       "      <td>2.304572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.002048</td>\n",
       "      <td>2.312763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.002160</td>\n",
       "      <td>2.317084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.002293</td>\n",
       "      <td>2.319376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.002320</td>\n",
       "      <td>2.351862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.002326</td>\n",
       "      <td>2.354189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.002361</td>\n",
       "      <td>2.356550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.002419</td>\n",
       "      <td>2.361388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.002420</td>\n",
       "      <td>2.366229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.002541</td>\n",
       "      <td>2.378933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.002551</td>\n",
       "      <td>2.381484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.002615</td>\n",
       "      <td>2.391946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.002808</td>\n",
       "      <td>2.394754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.002934</td>\n",
       "      <td>2.406490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.003067</td>\n",
       "      <td>2.446365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.003265</td>\n",
       "      <td>2.452894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.003271</td>\n",
       "      <td>2.456166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.003334</td>\n",
       "      <td>2.459499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.003677</td>\n",
       "      <td>2.463177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.015368</td>\n",
       "      <td>2.478545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ccp_alphas  impurities\n",
       "0     0.000000    2.243664\n",
       "1     0.001094    2.244758\n",
       "2     0.001269    2.246027\n",
       "3     0.001350    2.247377\n",
       "4     0.001364    2.248741\n",
       "5     0.001372    2.250113\n",
       "6     0.001386    2.251499\n",
       "7     0.001403    2.252903\n",
       "8     0.001438    2.254340\n",
       "9     0.001444    2.258672\n",
       "10    0.001586    2.261844\n",
       "11    0.001636    2.265116\n",
       "12    0.001676    2.266792\n",
       "13    0.001693    2.268485\n",
       "14    0.001702    2.270187\n",
       "15    0.001727    2.273642\n",
       "16    0.001732    2.275374\n",
       "17    0.001767    2.277141\n",
       "18    0.001790    2.278931\n",
       "19    0.001838    2.280769\n",
       "20    0.001909    2.282678\n",
       "21    0.001912    2.286501\n",
       "22    0.001947    2.290395\n",
       "23    0.002016    2.292411\n",
       "24    0.002025    2.296462\n",
       "25    0.002028    2.304572\n",
       "26    0.002048    2.312763\n",
       "27    0.002160    2.317084\n",
       "28    0.002293    2.319376\n",
       "29    0.002320    2.351862\n",
       "30    0.002326    2.354189\n",
       "31    0.002361    2.356550\n",
       "32    0.002419    2.361388\n",
       "33    0.002420    2.366229\n",
       "34    0.002541    2.378933\n",
       "35    0.002551    2.381484\n",
       "36    0.002615    2.391946\n",
       "37    0.002808    2.394754\n",
       "38    0.002934    2.406490\n",
       "39    0.003067    2.446365\n",
       "40    0.003265    2.452894\n",
       "41    0.003271    2.456166\n",
       "42    0.003334    2.459499\n",
       "43    0.003677    2.463177\n",
       "44    0.015368    2.478545"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(trees.cost_complexity_pruning_path( y_basic_train, model_X_basic_train ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees_fit =  trees.fit( y_basic_train, model_X_basic_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function flush_figures at 0x0000025B7D0DC790> (for post_execute):\n"
     ]
    }
   ],
   "source": [
    "# tree\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree( trees_fit , filled = True , rounded = True  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046456,
     "end_time": "2021-02-13T18:20:10.328795",
     "exception": false,
     "start_time": "2021-02-13T18:20:10.282339",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "An important method to improve predictive performance is called \"Pruning the Tree\". This\n",
    "means the process of cutting down the branches of a tree. We apply pruning to the complex tree above to reduce the depth. Initially, we determine the optimal complexity of the regression tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_check_feature_names',\n",
       " '_check_n_features',\n",
       " '_compute_partial_dependence_recursion',\n",
       " '_estimator_type',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_more_tags',\n",
       " '_prune_tree',\n",
       " '_repr_html_',\n",
       " '_repr_html_inner',\n",
       " '_repr_mimebundle_',\n",
       " '_validate_X_predict',\n",
       " '_validate_data',\n",
       " 'apply',\n",
       " 'ccp_alpha',\n",
       " 'class_weight',\n",
       " 'cost_complexity_pruning_path',\n",
       " 'criterion',\n",
       " 'decision_path',\n",
       " 'feature_importances_',\n",
       " 'feature_names_in_',\n",
       " 'fit',\n",
       " 'get_depth',\n",
       " 'get_n_leaves',\n",
       " 'get_params',\n",
       " 'max_depth',\n",
       " 'max_features',\n",
       " 'max_features_',\n",
       " 'max_leaf_nodes',\n",
       " 'min_impurity_decrease',\n",
       " 'min_samples_leaf',\n",
       " 'min_samples_split',\n",
       " 'min_weight_fraction_leaf',\n",
       " 'n_features_',\n",
       " 'n_features_in_',\n",
       " 'n_outputs_',\n",
       " 'predict',\n",
       " 'random_state',\n",
       " 'score',\n",
       " 'set_params',\n",
       " 'splitter',\n",
       " 'tree_']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(trees_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.047157,
     "end_time": "2021-02-13T18:20:10.540327",
     "exception": false,
     "start_time": "2021-02-13T18:20:10.493170",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, we can prune the tree and visualize the prediction rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.052169,
     "end_time": "2021-02-13T18:20:11.234467",
     "exception": false,
     "start_time": "2021-02-13T18:20:11.182298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "E.g., in the pruned tree the predicted hourly log wage for high-school graduates with more than $9.5$ years of experience is $2.8$, and otherwise is $2.6$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04994,
     "end_time": "2021-02-13T18:20:11.334448",
     "exception": false,
     "start_time": "2021-02-13T18:20:11.284508",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Finally, we calculate the mean-squared error and the $R^2$ on the test sample to evaluate the out-of-sample performance of the pruned tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.052287,
     "end_time": "2021-02-13T18:20:11.566330",
     "exception": false,
     "start_time": "2021-02-13T18:20:11.514043",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Random Forest and Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.050794,
     "end_time": "2021-02-13T18:20:11.667980",
     "exception": false,
     "start_time": "2021-02-13T18:20:11.617186",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the next step, we apply the more advanced tree-based methods random forest and boosted trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.051225,
     "end_time": "2021-02-13T18:21:08.500313",
     "exception": false,
     "start_time": "2021-02-13T18:21:08.449088",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To conclude, let us have a look at our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.052403,
     "end_time": "2021-02-13T18:21:08.603976",
     "exception": false,
     "start_time": "2021-02-13T18:21:08.551573",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.052897,
     "end_time": "2021-02-13T18:21:08.930888",
     "exception": false,
     "start_time": "2021-02-13T18:21:08.877991",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Above, we displayed the results for a single split of data into the training and testing part. The table shows the test MSE in column 1 as well as the standard error in column 2 and the test $R^2$\n",
    "in column 3. We see that the prediction rule produced by Elastic Net using the flexible model performs the best here, giving the lowest test MSE. Cross-Validated Lasso and Ridge, perform nearly as well. For any two of these methods, their testing MSEs are within one standard error of each other. Remarkably, OLS on a simple model performs extremely well, almost as well as best tree based method Random Forest. On the other hand, OLS on a flexible model with many regressors performs very poorly giving the highest test MSE. It is worth to notice that the nonlinear models, e.g. Random Forest, are not tuned. Thus, there is a lot of potential to improve the performance of the nonlinear methods we used in the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.052594,
     "end_time": "2021-02-13T18:21:09.036009",
     "exception": false,
     "start_time": "2021-02-13T18:21:08.983415",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.053134,
     "end_time": "2021-02-13T18:21:09.146558",
     "exception": false,
     "start_time": "2021-02-13T18:21:09.093424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the final step, we can build a prediction model by combing the strengths of the models we considered so far. This ensemble method is of the form\n",
    "\t$$ f(x) = \\sum_{k=1}^K \\alpha_k f_k(x) $$\n",
    "where the $f_k$'s denote our prediction rules from the table above and the $\\alpha_k$'s are the corresponding weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.053696,
     "end_time": "2021-02-13T18:21:09.254342",
     "exception": false,
     "start_time": "2021-02-13T18:21:09.200646",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We focus on the prediction rules based on OLS, Post-Lasso, Elastic Net, Pruned Tree, Random Forest, Boosted Trees, and Neural Network and combine these methods into an ensemble method. The weights can be determined by a simple ols regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.054822,
     "end_time": "2021-02-13T18:21:09.498067",
     "exception": false,
     "start_time": "2021-02-13T18:21:09.443245",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Alternatively, we can determine the weights via lasso regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.055874,
     "end_time": "2021-02-13T18:21:09.838636",
     "exception": false,
     "start_time": "2021-02-13T18:21:09.782762",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The estimated weights are shown in the following table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.056002,
     "end_time": "2021-02-13T18:21:10.101284",
     "exception": false,
     "start_time": "2021-02-13T18:21:10.045282",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Further, the $R^2$ for the test sample gets improved from $30\\%$ obtained by OLS to about $31\\%$ obtained by the ensemble method. We see that it is very powerful to aggregate prediction rules into an ensemble rule. Nevertheless, it is worth to notice that we should compare the ensemble method and the single rules on an additional validation set to ensure a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>S_E_ for MSE</th>\n",
       "      <th>R-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Least Squares (basic)</th>\n",
       "      <td>0.232</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Least Squares (flexible)</th>\n",
       "      <td>0.242</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.236</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Post-Lasso</th>\n",
       "      <td>0.237</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso (flexible)</th>\n",
       "      <td>0.236</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Post-Lasso (flexible)</th>\n",
       "      <td>0.239</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cross-Validated lasso</th>\n",
       "      <td>0.231</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cross-Validated ridge</th>\n",
       "      <td>0.318</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cross-Validated elnet</th>\n",
       "      <td>0.232</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cross-Validated lasso (flexible)</th>\n",
       "      <td>0.231</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cross-Validated ridge (flexible)</th>\n",
       "      <td>0.316</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cross-Validated elnet (flexible)</th>\n",
       "      <td>0.235</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Boosted Trees</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pruned Tree</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    MSE  S_E_ for MSE  R-squared\n",
       "Least Squares (basic)             0.232         0.017      0.273\n",
       "Least Squares (flexible)          0.242         0.017      0.241\n",
       "Lasso                             0.236         0.017      0.259\n",
       "Post-Lasso                        0.237         0.017      0.256\n",
       "Lasso (flexible)                  0.236         0.016      0.259\n",
       "Post-Lasso (flexible)             0.239         0.017      0.250\n",
       "Cross-Validated lasso             0.231         0.017      0.002\n",
       "Cross-Validated ridge             0.318         0.018      0.275\n",
       "Cross-Validated elnet             0.232         0.017      0.272\n",
       "Cross-Validated lasso (flexible)  0.231         0.017      0.008\n",
       "Cross-Validated ridge (flexible)  0.316         0.018      0.275\n",
       "Cross-Validated elnet (flexible)  0.235         0.017      0.262\n",
       "Random Forest                     0.000         0.000      0.000\n",
       "Boosted Trees                     0.000         0.000      0.000\n",
       "Pruned Tree                       0.000         0.000      0.000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table= np.zeros( (15, 3) )\n",
    "table[0,0:2]   = MSE_lm_basic\n",
    "table[1,0:2]   = MSE_lm_flex\n",
    "table[2,0:2]   = MSE_lasso.iloc[0, [0, 1]]\n",
    "table[3,0:2]   = MSE_lasso_post.iloc[0, [0, 1]]\n",
    "table[4,0:2]   = MSE_lasso_flex.iloc[0, [0, 1]]\n",
    "table[5,0:2]   = MSE_lasso_post_flex.iloc[0, [0, 1]]\n",
    "table[6,0:2]   = MSE_lasso_cv_basic.iloc[0, [0, 1]]\n",
    "table[7,0:2]   = MSE_ridge_basic.iloc[0, [0, 1]]\n",
    "table[8,0:2]   = MSE_elnet_basic.iloc[0, [0, 1]]\n",
    "table[9,0:2]   = MSE_lasso_cv_flex.iloc[0, [0, 1]]\n",
    "table[10,0:2]  = MSE_ridge_flex.iloc[0, [0, 1]]\n",
    "table[11,0:2]  = MSE_elnet_flex.iloc[0, [0, 1]]\n",
    "# table[13,1:2]  = MSE_rf\n",
    "# table[14,1:2]  = MSE_boost\n",
    "# table[15,1:2]  = MSE_pt\n",
    "\n",
    "\n",
    "\n",
    "table[0,2]   = R2_lm_basic\n",
    "table[1,2]   = R2_lm_flex\n",
    "table[2,2]   = R2_lasso\n",
    "table[3,2]   = R2_lasso_post\n",
    "table[4,2]   = R2_lasso_flex\n",
    "table[5,2]   = R2_lasso_post_flex\n",
    "table[6,2]   = R2_lasso_cv_basic\n",
    "table[7,2]   = R2_ridge_basic\n",
    "table[8,2]   = R2_elnet_basic\n",
    "table[9,2]   = R2_lasso_cv_flex\n",
    "table[10,2]  = R2_ridge_flex\n",
    "table[11,2]  = R2_elnet_flex\n",
    "# table[13,3]  = R2_rf\n",
    "# table[14,3]  = R2_boost\n",
    "# table[15,3]  = R2_pt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "colnames_table= [\"MSE\", \"S_E_ for MSE\", \"R-squared\"]\n",
    "rownames_table= [\"Least Squares (basic)\",\"Least Squares (flexible)\", \"Lasso\", \"Post-Lasso\",\"Lasso (flexible)\",\"Post-Lasso (flexible)\", \\\n",
    "                    \"Cross-Validated lasso\", \"Cross-Validated ridge\",\"Cross-Validated elnet\",\"Cross-Validated lasso (flexible)\",\"Cross-Validated ridge (flexible)\",\"Cross-Validated elnet (flexible)\",  \\\n",
    "                    \"Random Forest\",\"Boosted Trees\", \"Pruned Tree\"]\n",
    "table_pandas = pd.DataFrame( table, columns = colnames_table )\n",
    "table_pandas.index = rownames_table\n",
    "\n",
    "table_pandas = table_pandas.round(3)\n",
    "table_html = table_pandas.to_latex()\n",
    "table_pandas"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}